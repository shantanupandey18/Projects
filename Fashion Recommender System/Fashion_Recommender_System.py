# -*- coding: utf-8 -*-
"""MIS 584 Team 4 Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u066KMzOpvOsLLSHnwQaG6tRzCH18J9q

# MIS 584 Team 4 Final Project
by Jared Dorman, Yinya "Grace" Huang, Aadhithya Dinesh, Qianru Li, and Shantanu Pandey
"""

!pip install pandas
!pip install numpy
!pip install matplotlib
!pip install seaborn
!pip install scipy
!pip install plotly

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import plotly.express as px
# %matplotlib inline

"""## Reading the data

Grace will put the data in the Google Drive.
"""

!pip install google-colab

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/', force_remount=True)

# %cd "/content/drive/MyDrive/MIS 584 Project/"

# Commented out IPython magic to ensure Python compatibility.
# %cd '\Documents\MIS 584 Final Project'

articles = pd.read_csv("articles.csv")
articles.shape

articles.head()

articles.dtypes

customers = pd.read_csv("customers.csv")
customers.shape

customers.head()

customers.dtypes

training = pd.read_csv("transactions_train.csv")
training.shape

training.head(10)

training.dtypes

testing = pd.read_csv("sample_submission.csv")
testing.shape

testing.iloc[0, 1]

testing.dtypes

"""## Cleaning the data"""

# there are na's in the detail_desc column of articles
articles_nas = articles.isna().sum()
articles_nas

# if there is no detail_desc, use the prod_name + product_type_name as the substitute
no_detail_desc = articles['detail_desc'].isna()
articles.loc[no_detail_desc, 'detail_desc'] = articles[no_detail_desc]['prod_name'] + " "+ articles[no_detail_desc]['product_type_name']
articles[no_detail_desc].head()

# we will use the 'article_text' column for the text embeddings
articles['article_text'] = articles['colour_group_name'] + " " + \
articles['graphical_appearance_name'] + " " + articles['detail_desc']
articles['article_text'].head()

# it is okay to have na's for FN, Active, club_member_status,
# fashion_news_frequency, and age because we don't need these to make outfit
# recommendations
customers_nas = customers.isna().sum()
customers_nas

# none of our training data has na's so that is good
training_nas = training.isna().sum()
training_nas

# no na's in our testing data either
testing_nas = testing.isna().sum()
testing_nas

training_agg = training.drop(['price', 'sales_channel_id'], axis=1).groupby(['customer_id',	'article_id']).agg({'t_dat': 'count'}).reset_index()\
  .rename(columns={'t_dat': 'purchase_count'})
training_agg

# the data is skewed to the right
# min: 1, max: 570
training_agg['purchase_count'].describe()

def normalize(column):
    upper = column.max()
    y = column/upper
    return y

# min-max normalize the purchase_count column
training_agg['purchase_count_normalized'] = normalize(training_agg['purchase_count'])
training_agg

training_agg['purchase_count_normalized'].describe()

training_agg = training_agg.drop(['purchase_count'], axis = 1)
training_agg

# create a new column where the prediction is a list of integers and drop the old column
testing['prediction list'] = [list(map(int, i.split())) for i in testing['prediction']]
testing = testing.drop('prediction', axis=1)
testing

# the model should produce n=12 items
len(testing.iloc[0, 1])

testing_new = testing['prediction list'].apply(pd.Series) \
   .merge(testing, right_index = True, left_index = True) \
   .drop(['prediction list'], axis = 1) \
   .melt(id_vars = ['customer_id'], value_name = 'article_id')\
   .sort_values(by=['customer_id', 'variable'])
testing_new

"""## Exploring the data

***1. Articles Exploration***
"""

# count of unique articles
pd.Series.nunique(articles['article_id'])

# Top 10 product names
product_name = articles['prod_name'].value_counts()
print('Top 10 product names:\n', product_name.head(10), end='\n'*2)
# Visulizaiton of Top 10s
prod_name = articles["prod_name"].value_counts()
prod_name[:10].plot.bar(figsize=(15,5),rot=60,color='tomato',title='Top 10 Product Name')

# Top 10 product types
prod_type_name = articles['product_type_name'].value_counts()
print('Top 10 product types:\n',prod_type_name.head(10),end='\n'*2)
# Visulization of Top10
type_name = articles["product_type_name"].value_counts()
type_name[:10].plot.bar(figsize=(15,5),rot=0,color='darkcyan',title='Top 10 Product Type Name')

# Top 10 product_group_name
product_group_name = articles['product_group_name'].value_counts()
print('Top 10 product group names:\n',product_group_name.head(10),end='\n'*2)
# Visulization of top10
group_name = articles["product_group_name"].value_counts()
group_name[:10].plot.bar(figsize=(15,5),rot=60,color='gold',title='Top 10 Product Group Name')

# Top 10 colour_group_name
colour_group_name = articles['colour_group_name'].value_counts()
print('Top 10 colour group names:\n',colour_group_name.head(10),end='\n'*2)
# Visulization of Top10
colgroup_name = articles["colour_group_name"].value_counts()
colgroup_name[:10].plot.bar(figsize=(15,5),rot=0,color='royalblue',title='Top 10 Colour Group Name')

# index_group structure
articles_index_group_str = articles.groupby(['index_group_name', 'index_name']).count()['article_id']
print('Subgroup of index group:\n',articles_index_group_str.sort_values(ascending=False),end='\n'*2)
# product_group structure
articles_product_group_str = articles.groupby(['product_group_name', 'product_type_name']).count()['article_id']
print('Subgroup of product group:\n',articles_product_group_str,end='\n'*2)

# articles with the same product code can have different article id's because
# of variations in color, print, etc.
count_product_group = articles.groupby('product_group_name').agg({
    'article_id': pd.Series.nunique,
    'product_code': pd.Series.nunique
})
count_product_group

graphical_appearance = articles.groupby('graphical_appearance_name')\
.agg({
  'article_id': pd.Series.nunique
}).sort_values('article_id',ascending=False)
graphical_appearance

color = articles.groupby('colour_group_name').agg({
    'article_id': pd.Series.nunique
}).sort_values('article_id', ascending=False)
color

""":***2. Customers Exploration***"""

pd.Series.nunique(customers['customer_id'])

sns.set_style("whitegrid")
age = plt.subplots(figsize=(10,5))
age = sns.histplot(data=customers, x='age', bins=50, color='blue')
age.set_xlabel('Distribution of the customers age')
plt.show()

sns.set_style("whitegrid")
status = plt.subplots(figsize=(10,5))
status = sns.histplot(data=customers, x='club_member_status', color='orange')
status.set_xlabel('Distribution of club member status')
plt.show()

sns.set_style("whitegrid")
frequency = plt.subplots(figsize=(10,5))
frequency = sns.histplot(data=customers, x='fashion_news_frequency', color='red')
frequency.set_xlabel('Distribution of fashion news frequency')
plt.show()

"""***1. Transactions(Training) Exploration***"""

print("Top 10 purchased articles: ")
training.article_id.value_counts().head(10)

cus_transactions = training.groupby('customer_id').count()
print("Top 10 Customers by Number of Items Purchased: ")
cus_transactions.sort_values(by='price', ascending=False)['price'][:10]

merged_at = articles[['article_id', 'prod_name', 'product_type_name', 'product_group_name', 'colour_group_name']]
merged_at = training.merge(merged_at, on='article_id', how='left')

df = merged_at[['product_type_name', 'price']].groupby('product_type_name').sum()
sales_per_product=df.sort_values("price", axis = 0, ascending = False)
sales_per_product.columns =['Sales']
sales_per_product.loc[:, "Sales"] = sales_per_product["Sales"].map('{:.0f}'.format)
print("Top 10 product type by sales: ")
sales_per_product.head(10)

dfn = merged_at[['colour_group_name', 'price']].groupby('colour_group_name').sum()
sales_per_col=dfn.sort_values("price", axis = 0, ascending = False)
sales_per_col.columns =['Sales']
sales_per_col.loc[:, "Sales"] = sales_per_col["Sales"].map('{:.0f}'.format)
print("Top 10 color group by sales: ")
sales_per_col.head(10)

plt.figure(figsize=(16, 9))
plt.title('Purchases Per Day')
training.groupby(["t_dat"])["article_id"].count().plot()

"""## Collaborative Filtering Recommendation System using Turicreate Package"""

# !pip install turicreate

import time
import turicreate as tc
import sys
sys.path.append("..")

user_id = 'customer_id'
item_id = 'article_id'
users_to_recommend = list(testing[user_id])
n_rec = 12
n_display = 30
# training_sframe = tc.SFrame(training_agg)
# testing_sframe = tc.SFrame(testing_new)

# # old training code
# def model(train_data, name, user_id, item_id, target, users_to_recommend, n_rec, n_display):
#   if name == 'popularity': model = tc.popularity_recommender.create(train_data,
#                                                     user_id=user_id,
#                                                     item_id=item_id,
#                                                     target=target)
#   elif name == 'cosine': model = tc.item_similarity_recommender.create(train_data,
#                                                     user_id=user_id,
#                                                     item_id=item_id,
#                                                     target=target,
#                                                     similarity_type='cosine')
#   elif name == 'pearson': model = tc.item_similarity_recommender.create(train_data,
#                                                     user_id=user_id,
#                                                     item_id=item_id,
#                                                     target=target,
#                                                     similarity_type='pearson')

#   recom = model.recommend(users=users_to_recommend, k=n_rec)
#   recom.print_rows(n_display)
#   return model

# don't run this unless you want to wait forever
# this is how you train the model and get recommendations
# name = 'cosine'
# target = 'purchase_count'
# model = tc.item_similarity_recommender.create(training_sframe, user_id=user_id,
#                                               item_id=item_id, target=target,
#                                               similarity_type='cosine')
recom = model.recommend(users=users_to_recommend, k=n_rec)
recom.print_rows(n_display)

recommendations = recom.to_dataframe()
print(recommendations.shape)
recommendations.head()

recommendations.to_csv("predicted_recommendations_11-18-2022.csv")

# # old training code
# name = 'cosine'
# target = 'purchase_count'
# cos = model(training_sframe, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)

tc.evaluation.accuracy(testing_sframe['article_id'],recom['article_id'], average='micro')

"""## Item-Based Collaborative Filtering Using Surprise Package

"""

!pip install scikit-surprise

from surprise import Dataset
from surprise import NormalPredictor
from surprise import Reader
from surprise import SVD
from surprise import SVDpp
from surprise import KNNBasic
from surprise import accuracy
from surprise.model_selection import cross_validate

reader = Reader(rating_scale=(0, 1))

data = Dataset.load_from_df(training_agg[["customer_id", "article_id", "purchase_count_normalized"]], reader)

training_surprise = data.build_full_trainset()

sim_options = {'name': 'cosine',
               'user_based': False  # compute  similarities between items
               }
algo = KNNBasic(sim_options=sim_options)

# Train the algorithm on the trainset, and predict ratings for the testset
algo.fit(training_surprise)
predictions = algo.test(testing_new)

# Then compute RMSE, MSE and MAE
print(accuracy.rmse(predictions))
print(accuracy.mse(predictions))
print(accuracy.mae(predictions))