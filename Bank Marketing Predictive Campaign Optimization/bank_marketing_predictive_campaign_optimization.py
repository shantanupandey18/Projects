# -*- coding: utf-8 -*-
"""Bank_Marketing_Predictive_Campaign_Optimization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dj3TqVsFRpJ-DLK6KaEtZ_dbP9bhqy5U

## Part 0: Summary
[Bank Marketing dataset](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) hosted on the UCI machine learning repository.

For the banking industry, an important task is to market their products (e.g., a term deposit or a credit card) to potential customers. However, such tasks are usually challenging as banks need to cautiously balance the cost of large-scale marketing campaigns and the profit of signing up more customers.

To address this issue, machine learning models have been widely adopted by the banking industry to identify potential customers and improve marketing effectiveness. The task is to develop machine learning models to predict whether a customer would sign up a term deposit using various features collected by a bank. Evaluate the performance of each model and recommend the most preferred model to the stakeholders in the marketing department.

The `bank-train.csv` includes information on 32158 customers and the `bank-test.csv` includes information on another 8040 customers. For both datasets, there are 11 features that you can use for prediction. Below we list the detailed definitions for each feature:
* age: age of the customer
* housing: whether the customer has housing loan (0 for no; 1 for yes)
* loan: whether the customer has personal loan (0 for no; 1 for yes)
* contact: contact communication type (0 for cellular; 1 for telephone)
* campaign: number of contacts performed during this campaign and for this customer
* previous: number of contacts performed before this campaign and for this customer
* emp.var.rate: employment variation rate - quarterly indicator
* cons.price.idx: consumer price index - monthly indicato
* cons.conf.idx: consumer confidence index - monthly indicator
* euribor3m: euribor 3 month rate - daily indicator
* nr.employed: number of employees - quarterly indicator

The label we are going to predict has the name `y`, which indicates whether the customer signed up for the term deposit or not (0 for no; 1 for yes).
"""

from urllib.request import urlretrieve
urlretrieve('https://drive.google.com/uc?export=download&id=16ECL47eCqWvWvXFZWxQak06L93oeUMP-',
            'bank-train.csv')
urlretrieve('https://drive.google.com/uc?export=download&id=1xDumIeMWoI4w82YKEvhflwZLSzqHOq-f',
            'bank-test.csv')

"""## Section 1: Import and Process Data

"""

import pandas as pd

# read the dataset
data = pd.read_csv('bank-train.csv', sep=',')
data_test = pd.read_csv('bank-test.csv', sep=',')

# show the dataset
## list the shape of the dataset in training
print('Shape of the dataset: ')
print(data.shape)
print()

## list the shape of the dataset in testing
print('Shape of testing dataset')
print(data_test.shape)
print()

## summarize the training dataset
print('Summary of the dataset:')
print(data.describe())
print()

## summarize the testing dataset
print('Summary of the dataset:')
print(data_test.describe())
print()


# create training data and labels
X = data.drop(columns='y')
y = data['y']

# create  testing data and labels
X_t = data_test.drop(columns='y')
y_t = data_test['y']

"""## Section 2: Apply Machine Learning Classification Methods
In this section, we train and evaluate various machine learning classification methods.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score,\
    precision_score, f1_score, roc_auc_score

# Training the dataset
knn = KNeighborsClassifier(n_neighbors=5)           # K = 5
knn.fit(X, y)                                       # fit the model
knn_pred_t = knn.predict(X_t)                       # make predictions
knn_score_t = knn.predict_proba(X_t)                # get prediction scores

## print the predicted labels
print('Predicted labels testing:')
print(knn_pred_t)
print()

## print the prediction scores
print('Predicted scores testing:')
print(knn_score_t)
print()


# calculate prediction performance
print('Confusion Matrix testing:')
knn_conf_mat = confusion_matrix(y_t, knn_pred_t)
print(knn_conf_mat)
print()

## accuracy
knn_acc = accuracy_score(y_t, knn_pred_t)
print('Prediction accuracy testing dataset: {:.4f}'.format(knn_acc))

## recall
knn_recall = recall_score(y_t, knn_pred_t)
print('Prediction recall testing dataset: {:.4f}'.format(knn_recall))

## precision
knn_precision = precision_score(y_t, knn_pred_t)
print('Prediction precision testing dataset: {:.4f}'.format(knn_precision))

## F1 score
knn_f1 = f1_score(y_t, knn_pred_t)
print('Prediction F1 testing dataset: {:.4f}'.format(knn_f1))

## AUC-ROC
knn_auc = roc_auc_score(y_t, knn_score_t[:, 1])
print('AUC-ROC testing dataset: {:.4f}'.format(knn_auc))
print()

from sklearn.naive_bayes import GaussianNB

# Model ran
gnb = GaussianNB()
gnb.fit(X, y)                                       # fit the model
gnb_pred_t = gnb.predict(X_t)                       # make predictions
gnb_score_t = gnb.predict_proba(X_t)                # get prediction scores

## accuracy
gnb_acc = accuracy_score(y_t, gnb_pred_t)
print('Prediction accuracy: {:.4f}'.format(gnb_acc))

## recall
gnb_recall = recall_score(y_t, gnb_pred_t)
print('Prediction recall: {:.4f}'.format(gnb_recall))

## precision
gnb_precision = precision_score(y_t, gnb_pred_t)
print('Prediction precision: {:.4f}'.format(gnb_precision))

## F1 score
gnb_f1 = f1_score(y_t, gnb_pred_t)
print('Prediction F1: {:.4f}'.format(gnb_f1))

## AUC-ROC
gnb_auc = roc_auc_score(y_t, gnb_score_t[:, 1])
print('AUC-ROC : {:.4f}'.format(gnb_auc))
print()

from sklearn.linear_model import LogisticRegression

# Model ran
log_clf = LogisticRegression()
log_clf.fit(X, y)                                        # fit the model
log_clf_pred_t = log_clf.predict(X_t)                    # make predictions
log_clf_score_t = log_clf.predict_proba(X_t)             # get prediction scores

## accuracy
log_clf_acc = accuracy_score(y_t, log_clf_pred_t)
print('Prediction accuracy testing: {:.4f}'.format(log_clf_acc))

## recall
log_clf_recall = recall_score(y_t, log_clf_pred_t)
print('Prediction recall testing: {:.4f}'.format(log_clf_recall))

## precision
log_clf_precision = precision_score(y_t, log_clf_pred_t)
print('Prediction precision testing: {:.4f}'.format(log_clf_precision))

## F1 score
log_clf_f1 = f1_score(y_t, log_clf_pred_t)
print('Prediction F1 testing: {:.4f}'.format(log_clf_f1))

## AUC-ROC
log_clf_auc = roc_auc_score(y_t, log_clf_score_t[:, 1])
print('AUC-ROC testing: {:.4f}'.format(log_clf_auc))
print()

# you can use the code below to extract logistic regression's features and their
# coefficients; specifically, feature_names_in_ will return all the input features,
# and coef_ will return all the features' estimated coefficients
log_clf_coef = pd.DataFrame({
    'Feature Name': log_clf.feature_names_in_,
    'Coefficient': log_clf.coef_[0]
})
print(log_clf_coef)

"""Explaination for Logistic Regression Coefficients:
Age, cons.conf.index have coefficients close to zero but are postitve so the slope is positive.
Housing, loan,campaign,nr.employed have coefficients close to zero but are negative so the slope is negative.
Contact,emp.var.rate,euribor3m have negative coefficients but there value is less than 0.5 so the slope is not too steep.
previous has positive coefficients but the value is less than 0.5 so the slope is not too steep.
cons.price.index has the largest coefficient and is positive as well.

cons.price.index: we can deduce that for change in one unit of cons.price.index the "y" will see an increase by the factor of 0.4 keeping other variables constant.

Age:  for change in one unit of Age the "y" will see an increase by the factor of 0.0018 keeping other variables constant.

Housing:  for change in one unit of Housing the "y" will see a decrease by the factor of 0.007 keeping other variables constant.

Loan : for change in one unit of Loan the "y" will see a decrease by the factor of 0.011 keeping other variables constant.

Contact: for change in one unit of contact the "y" will see a decrease by the factor of 0.13 keeping other variables constant.

Campaign: for change in one unit of campaign the "y" will see a decrease by the factor of 0.04 keeping other variables constant.

Previous: for change in one unit of previous the "y" will see an increase by the factor of 0.14 keeping other variables constant.

emp.var.rate: for change in one unit of emp.var.rate the "y" will see a decrease by the factor of 0.12 keeping other variables constant.

cons.conf.index: for change in one unit of cons.conf.index the "y" will see an increase by the factor of 0.04 keeping other variables constant.

euribor3m: for change in one unit of euribor3m the "y" will see a decrease by the factor of 0.17 keeping other variables constant.

nr.employes: for change in one unit of nr.employes the "y" will see a decrease by the factor of 0.007 keeping other variables constant.



"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score,\
    precision_score, f1_score, roc_auc_score

# Ran model
dt_clf = DecisionTreeClassifier(max_depth=5)
dt_clf.fit(X, y)                                    # fit the model
dt_clf_pred_t = dt_clf.predict(X_t)                 # make predictions
dt_clf_score_t = dt_clf.predict_proba(X_t)          # get prediction scores

print(dt_clf_pred_t)

print(dt_clf_score_t)

## accuracy
dt_clf_acc = accuracy_score(y_t, dt_clf_pred_t)
print('Prediction accuracy for testing: {:.4f}'.format(dt_clf_acc))

## recall
dt_clf_recall = recall_score(y_t, dt_clf_pred_t)
print('Prediction recall for testing: {:.4f}'.format(dt_clf_recall))

## precision
dt_clf_precision = precision_score(y_t, dt_clf_pred_t)
print('Prediction precision testing: {:.4f}'.format(dt_clf_precision))

## F1 score
dt_clf_f1 = f1_score(y_t, dt_clf_pred_t)
print('Prediction F1 testing: {:.4f}'.format(dt_clf_f1))

# auc-roc
dt_clf_auc = roc_auc_score(y_t, dt_clf_score_t[:, 1])
print('AUC-ROC testing: {:.4f}'.format(dt_clf_auc))

"""## Section 3: Summary & Recommendation

We can check for imbalance in the dataset to understand if the difference difference in number of people who signed up deposit and number of people who did not sign up is too large and work on methods to reduce the imbalance.

Based on the above regression using different methods it is clear that Decision Tree Regression Model provides the highest accuracy but since the dataset suffers from imbalance we should make the decision based on F1-Score which is highest for Naive Bayes(0.375).
"""