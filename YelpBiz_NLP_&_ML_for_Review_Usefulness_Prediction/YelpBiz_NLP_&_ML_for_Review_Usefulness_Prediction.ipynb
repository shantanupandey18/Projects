{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Process and Analyze Yelp Review Dataset (1 point)\n",
        "The Yelp dataset on user reviews of Tuscon businesses. Specifically, this dataset has six attributes whose descriptions are listed below:\n",
        "* `review_id`: a string-typed attribute indicating a review's ID\n",
        "* `user_id`: a string-typed attribute indicating the reviewer's ID\n",
        "* `business_id`: a string-typed attribute indicating the ID of the business that is reviewed\n",
        "* `review_stars`: a float-typed attribute indicating the review's star rating\n",
        "* `useful`: an integer-typed attribute indicating how many useful votes the review has received\n",
        "* `review_text`: a string-typed attribute storing the review's text"
      ],
      "metadata": {
        "id": "xuSEDEkvspM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "\n",
        "urlretrieve('https://drive.google.com/uc?export=download&id=11okLSqOwXBgOLw2lKXb0Bi_yMzoeC5u1',\n",
        "            'user_reviews.csv')"
      ],
      "metadata": {
        "id": "oivoO9fjOcRS",
        "outputId": "16ce442a-9b08-4971-d49f-a726930e7fa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('user_reviews.csv', <http.client.HTTPMessage at 0x7fbf4c6e89d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read csv\n",
        "import pandas as pd\n",
        "data = pd.read_csv('user_reviews.csv', sep=',')\n",
        "\n",
        "# Print dataframes\n",
        "print(data)\n",
        "\n",
        "# Remove duplicates & null for business id and review id\n",
        "data = data.dropna(axis = \"index\", how = \"any\").drop_duplicates(subset=[\"review_id\", \"business_id\"], keep=\"last\");\n",
        "\n",
        "# list top 20 users with most reviews\n",
        "df_metrics = data.groupby(['user_id']).agg(review_count = ('review_id', 'count'),\n",
        "                                           useful_count = ('useful', 'count'),\n",
        "                                           avg_stars = ('review_stars', 'mean')).sort_values(by = 'review_count',\n",
        "                                                                                           ascending = False).head(20)\n",
        "\n",
        "print(df_metrics)\n",
        "\n",
        "# List top 20 businesses\n",
        "df_business_metrics = data.groupby(['business_id']).agg(review_count = ('review_id', 'count'),\n",
        "                                                        avg_stars = ('review_stars', 'mean')).sort_values(by = 'review_count',\n",
        "                                                                                           ascending = False).head(20)\n",
        "print(df_business_metrics)"
      ],
      "metadata": {
        "id": "KAQR9vtymsFh",
        "outputId": "e9fbc14a-81ba-4786-f64f-4d45ece617e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   review_id                 user_id             business_id  \\\n",
            "0     FTcRb7TUjE-K6spSjs-0TA  CMYCfKoEu0WF9_43zRgr8g  5Ce3lZksYVkCbrihqylVHQ   \n",
            "1     oyxS126nYDZOL0qwPa8how  CMYCfKoEu0WF9_43zRgr8g  CA5BOxKRDPGJgdUQ8OUOpw   \n",
            "2     KbFlOy2PN2dXBjdk4mpz8g  CMYCfKoEu0WF9_43zRgr8g  1MAQQhmUNU0uzHw3KhPczg   \n",
            "3     mslt0F7LpdBMQmKGkn-bAA  CMYCfKoEu0WF9_43zRgr8g  QXB4E78FXn3eotalXUG0bQ   \n",
            "4     5SGsoqgx8CBbw6bcrsi0NQ  CMYCfKoEu0WF9_43zRgr8g  M983OPfVRnwvG7zEOzykCA   \n",
            "...                      ...                     ...                     ...   \n",
            "1677  SlCkCFHvk9IoIJ8DkMP1CQ  f57_fAYbAplRfp_Ngse_Yw  K9rx41SBIAin33eOkmKa_g   \n",
            "1678  y-2TMGGuMJ04MmTkWyVQLw  f57_fAYbAplRfp_Ngse_Yw  ECz-ZdvK3B35NCVUD49bNg   \n",
            "1679  pEspgH7mV2aECdvi-QlvhQ  f57_fAYbAplRfp_Ngse_Yw  ECz-ZdvK3B35NCVUD49bNg   \n",
            "1680  UDU70OpXesfrVNDpfQqPFA  f57_fAYbAplRfp_Ngse_Yw  ECz-ZdvK3B35NCVUD49bNg   \n",
            "1681  LeuCf8C5kXL_-JP8hF-fog  f57_fAYbAplRfp_Ngse_Yw  vPJ_ggSczt2vS6DUZ0B-Ag   \n",
            "\n",
            "      review_stars  useful                                        review_text  \n",
            "0              5.0       2  We love this little restaurant! It's not as ov...  \n",
            "1              5.0       1  We came here for dinner this evening and was a...  \n",
            "2              1.0       4  Just a heads up the owner, Roya, will not give...  \n",
            "3              1.0       9  Came in to get my ends cleaned up, FIVE TO SIX...  \n",
            "4              4.0       0  love the atmosphere!!! The patio is the best a...  \n",
            "...            ...     ...                                                ...  \n",
            "1677           5.0       0  I know these guys get a bad rap, but they were...  \n",
            "1678           4.0       4  So I have been using Bluespan for about a year...  \n",
            "1679           4.0       4  Since the post, the Bluespan team has been ver...  \n",
            "1680           2.0       6  I have had Bluespan for a couple months. For t...  \n",
            "1681           1.0       1  We ordered in advance and it still took an add...  \n",
            "\n",
            "[1682 rows x 6 columns]\n",
            "                        review_count  useful_count  avg_stars\n",
            "user_id                                                      \n",
            "jn_dHhsCj2scx1951CKutA            18            18   3.944444\n",
            "8OHkSxQRVfmMu5uQATi83g            18            18   3.611111\n",
            "Azxo0oP96tot8QGruS4XZw            17            17   4.176471\n",
            "r0pPV4-xj1sD_uGXVYxOaw            17            17   3.117647\n",
            "nl8HXOlCwIJ86pYavbi5UQ            16            16   4.687500\n",
            "Gs4OijDfrHzAbocJ-YYGog            15            15   3.133333\n",
            "4JSSan6RPCM0eWa1D7-46g            15            15   2.666667\n",
            "LdRkF_b7pvkjrmy3ZKEiig            15            15   4.333333\n",
            "iQnNxuxBEGb5NJBBRK-LVQ            14            14   4.571429\n",
            "LPovj-Wa7xXoJKM3qRy1bA            14            14   3.571429\n",
            "rSTIGvVuQErecycQQe4svg            14            14   4.785714\n",
            "IpKpHOGCqLWibbiZrcBBcQ            13            13   3.692308\n",
            "kbmUOqtVz4rB64zUpFKtnA            13            13   3.769231\n",
            "AiVl1U26I_O4WqKnIZ73vw            13            13   4.615385\n",
            "lbMsbFedMjgjhWIZjMgfkQ            13            13   3.538462\n",
            "oiZUTKnsIilXwyN-HCK55w            13            13   4.153846\n",
            "gY4vkCBjLxH19RQ1osujJQ            13            13   4.076923\n",
            "TiGuWj_zBTHj0dBHqMVZHA            13            13   3.384615\n",
            "5mxUsaiLC_QcPIFjENrn9A            12            12   4.166667\n",
            "wl7VR72-u8ADycZAwGC8gQ            12            12   4.666667\n",
            "                        review_count  avg_stars\n",
            "business_id                                    \n",
            "UCMSWPqzXjd7QHq7v8PJjQ             8   4.375000\n",
            "ZYxGiEyHD17kd80zUzaOQA             8   4.250000\n",
            "3aY8m5w6UnxXbRMhEUPUMg             7   3.428571\n",
            "5s7I0Khg7ReVzfO7niJtKg             7   4.571429\n",
            "tV46IhCfHbsx_af-pMupiw             7   4.142857\n",
            "43MDfrU28FYjfpamNfL9GA             7   3.857143\n",
            "3StNEgKAwpCFR1q0urmJrw             6   4.333333\n",
            "KZA_HEOsBXf8dtrk9rqNJA             6   3.500000\n",
            "YSRM9nWQn40eg49tSiI-_Q             6   3.666667\n",
            "4r6N_Fhiwoqo_FqQ7Mm6mQ             6   3.000000\n",
            "muxda1cSVtplETqTfYVgZA             6   3.833333\n",
            "Eqfks4GEn5dsI4ZGiPrCVQ             5   4.600000\n",
            "LZzDvgfpkd4nI3E4L9wF1w             5   4.000000\n",
            "gQ5_wcFqhplc9xrnSCt6-Q             5   4.600000\n",
            "OxBZqeYH5xuusEPp9ml7-g             5   4.600000\n",
            "Rv8bW3pkzpi5dZu5ckbgtA             5   3.400000\n",
            "4QMIJJWQOh7zpEwOXhFJqw             5   5.000000\n",
            "dMGWB4TEfEhgInQefGimAw             5   4.200000\n",
            "dOOvB4HW-b0mWDpcvY5h-g             5   4.200000\n",
            "0ghzROkZWKWwQKDt1fkPAQ             5   4.800000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Preprocess Review Text\n",
        " We first tokenize each review text, lowercase each token, remove punctuations and stop words, and conduct stemming for each remaining token. After finishing the preprocessing, we combine all remaining tokens of a review back to a single string. Print the first 25 preprocessed reviews."
      ],
      "metadata": {
        "id": "w39sMs5aOhtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## download the punkt module\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "## get punctuations\n",
        "import string\n",
        "punctuations = string.punctuation\n",
        "\n",
        "## download stop words\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "## import stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_7kFUn4T3j-",
        "outputId": "1395d3b2-fb3c-4972-c7bf-5cf035513141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "usefulness = data[['review_text', 'useful']]\n",
        "\n",
        "new_train_msgs = []\n",
        "review_tokenized_list = []\n",
        "review_lowercased = []\n",
        "review_tokenized_lowercase_list = []\n",
        "print('Result for nltk tokenization:')\n",
        "for msg in usefulness.review_text:\n",
        "    ## TODO: insert your code here to process each message in the spam-train.csv\n",
        "    review_tokenized_list.append(nltk.word_tokenize(msg))\n",
        "    review_lowercased.append(msg.lower())\n",
        "\n",
        "\n",
        "for labels in usefulness.review_text:\n",
        "    review_tokenized_lowercase_list.append(nltk.word_tokenize(labels))\n",
        "\n",
        "\n",
        "review_no_punctuations_words = []\n",
        "review_no_punctuations_list = []\n",
        "for words in review_tokenized_lowercase_list:\n",
        "    for word in words:\n",
        "        if word not in string.punctuation:\n",
        "            review_no_punctuations_words.append(word)\n",
        "    review_no_punctuations_list.append(review_no_punctuations_words)\n",
        "    review_no_punctuations_words = []\n",
        "\n",
        "# remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "review_no_stopwords = []\n",
        "review_no_stopwords_list = []\n",
        "for label in review_no_punctuations_list:\n",
        "    for word in label:\n",
        "      if word not in stop_words:\n",
        "          review_no_stopwords.append(word)\n",
        "    review_no_stopwords_list.append(review_no_stopwords)\n",
        "    review_no_stopwords = []\n",
        "\n",
        "# stemming the text\n",
        "review_stemmed = []\n",
        "review_stemmed_list_train = []\n",
        "for token in review_no_stopwords_list:\n",
        "  for word in token:\n",
        "    review_stemmed.append(ps.stem(word))\n",
        "  review_stemmed_list_train.append(review_stemmed)\n",
        "  review_stemmed = []\n",
        "\n",
        "### append the processed review into new list\n",
        "final_string_train = []\n",
        "for strings in review_stemmed_list_train:\n",
        "    review_processed_string = \" \".join(strings)\n",
        "    final_string_train.append(review_processed_string)\n",
        "    review_processed_string = \"\"\n",
        "\n",
        "# usefulness.review_text = final_string_train\n",
        "data.review_text = final_string_train\n",
        "\n",
        "print(data.review_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l0FvKRO61aM",
        "outputId": "be03d5f6-1a4d-470d-8574-0838c02343aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for nltk tokenization:\n",
            "0       love littl restaur 's overr loud compar place ...\n",
            "1       came dinner even absolut delici truli blown aw...\n",
            "2       head owner roya give refund respond client pai...\n",
            "3       came get end clean five six inch hair hack spe...\n",
            "4       love atmosph patio best area sit relax signif ...\n",
            "                              ...                        \n",
            "1677    know guy get bad rap open holiday weekend gave...\n",
            "1678    use bluespan year would recommend compani inte...\n",
            "1679    sinc post bluespan team respon fix issu short ...\n",
            "1680    bluespan coupl month past two week servic spot...\n",
            "1681    order advanc still took addit 20 minut get foo...\n",
            "Name: review_text, Length: 1488, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Predict Review Usefulness based on Their Text\n",
        "Finally, in this part, we select 2 appropriate machine learning models to predict each review's usefulness (i.e., number of useful votes) based on its text and report the models' prediction performance, namely KNN and Naive Bayes\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L9CATcyaPxT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = data['review_text']\n",
        "y = data['useful']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
        "                                   random_state=104,\n",
        "                                   test_size=0.25,\n",
        "                                   shuffle=True)\n"
      ],
      "metadata": {
        "id": "0hEu1gExi9Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can apply TfidfVectorizer function to extract TF-IDF vectors for each message\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_transformer = TfidfVectorizer(max_features=500)\n",
        "\n",
        "# TODO: replace the question marks '?' with the correct variables you created in Section 2.1\n",
        "tfidf_transformer.fit(x)\n",
        "train_features = tfidf_transformer.transform(x_train).toarray()\n",
        "test_features = tfidf_transformer.transform(x_test).toarray()\n",
        "\n",
        "print(train_features)\n",
        "print(test_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcM6B4zjx6DQ",
        "outputId": "4f0aef49-b073-45a9-edef-5822f956c146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.23968551 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.13283566 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score,\\\n",
        "    precision_score, f1_score, roc_auc_score\n",
        "\n",
        "# Training the dataset\n",
        "knn = KNeighborsClassifier(n_neighbors=5)           # K = 5\n",
        "knn.fit(train_features, y_train)                                       # fit the model\n",
        "knn_pred_t = knn.predict(test_features)                       # make predictions\n",
        "knn_score_t = knn.predict_proba(test_features)                # get prediction scores\n",
        "\n",
        "## print the predicted labels\n",
        "print('Predicted labels testing:')\n",
        "print(knn_pred_t)\n",
        "print()\n",
        "\n",
        "## print the prediction scores\n",
        "print('Predicted scores testing:')\n",
        "print(knn_score_t)\n",
        "print()\n",
        "\n",
        "\n",
        "# calculate prediction performance\n",
        "print('Confusion Matrix testing:')\n",
        "knn_conf_mat = confusion_matrix(y_test, knn_pred_t)\n",
        "print(knn_conf_mat)\n",
        "print()\n",
        "\n",
        "## accuracy\n",
        "knn_acc = accuracy_score(y_test, knn_pred_t)\n",
        "print('Prediction accuracy : {:.4f}'.format(knn_acc))\n",
        "\n",
        "## precision\n",
        "knn_precision = precision_score(y_test, knn_pred_t,average = \"weighted\")\n",
        "print('Prediction precision : {:.4f}'.format(knn_precision))\n",
        "\n",
        "## F1 score\n",
        "knn_f1 = f1_score(y_test, knn_pred_t,average = \"weighted\")\n",
        "print('Prediction F1 : {:.4f}'.format(knn_f1))\n"
      ],
      "metadata": {
        "id": "tSlCErkIPw4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab221bc-9911-4a1c-8345-0fe5bc6efb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels testing:\n",
            "[0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 2 0 0 1 1 0 0 0 0 0 2 0 1 0 1 1 0 0 0\n",
            " 0 1 1 3 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 9 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 0 2 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 2 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0\n",
            " 1 0]\n",
            "\n",
            "Predicted scores testing:\n",
            "[[0.6 0.4 0.  ... 0.  0.  0. ]\n",
            " [0.4 0.2 0.  ... 0.2 0.  0. ]\n",
            " [0.4 0.2 0.4 ... 0.  0.  0. ]\n",
            " ...\n",
            " [0.8 0.2 0.  ... 0.  0.  0. ]\n",
            " [0.  0.4 0.2 ... 0.  0.  0. ]\n",
            " [0.6 0.2 0.  ... 0.  0.  0. ]]\n",
            "\n",
            "Confusion Matrix testing:\n",
            "[[129  62   2   1   0   0   0   0   0   1   0   0   0]\n",
            " [ 60  30   1   0   0   0   0   0   0   0   0   0   0]\n",
            " [ 22  12   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [ 11  12   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  4   6   1   0   0   0   0   0   0   0   0   0   0]\n",
            " [  2   3   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  2   2   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  2   1   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  1   1   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   1   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   1   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   1   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   1   0   0   0   0   0   0   0   0   0   0   0]]\n",
            "\n",
            "Prediction accuracy : 0.4274\n",
            "Prediction precision : 0.3454\n",
            "Prediction F1 : 0.3815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score,\\\n",
        "    precision_score, f1_score, roc_auc_score\n",
        "\n",
        "# Model ran\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(train_features, y_train)                  # fit the model\n",
        "gnb_pred_t = gnb.predict(test_features)                       # make predictions\n",
        "gnb_score_t = gnb.predict_proba(test_features)                # get prediction scores\n",
        "\n",
        "## accuracy\n",
        "gnb_acc = accuracy_score(y_test, gnb_pred_t)\n",
        "print('Prediction accuracy: {:.4f}'.format(gnb_acc))\n",
        "\n",
        "## recall\n",
        "gnb_recall = recall_score(y_test, gnb_pred_t,average = \"weighted\")\n",
        "print('Prediction recall: {:.4f}'.format(gnb_recall))\n",
        "\n",
        "## precision\n",
        "gnb_precision = precision_score(y_test, gnb_pred_t,average = \"weighted\")\n",
        "print('Prediction precision: {:.4f}'.format(gnb_precision))\n",
        "\n",
        "## F1 score\n",
        "gnb_f1 = f1_score(y_test, gnb_pred_t,average = \"weighted\")\n",
        "print('Prediction F1: {:.4f}'.format(gnb_f1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHuFe9FuWvcj",
        "outputId": "ade1ee0b-e283-41a4-92e7-19c1c5c70bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction accuracy: 0.1909\n",
            "Prediction recall: 0.1909\n",
            "Prediction precision: 0.3371\n",
            "Prediction F1: 0.2274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}